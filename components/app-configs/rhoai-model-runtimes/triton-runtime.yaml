apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    opendatahub.io/template-enabled: "true"
    tags: triton-24.05,servingruntime
    argocd.argoproj.io/sync-wave: "2"
    description: Nvidia Triton Inference Server Runtime Definition
  labels:
    opendatahub.io/configurable: "true"
    opendatahub.io/dashboard: "true"
    # opendatahub.io/ootb: "true"
  name: triton
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: triton
      labels:
        name: triton
      annotations:
        maxLoadingConcurrency: "2"
        openshift.io/display-name: "Nvidia Triton Model Server"
    spec:
      supportedModelFormats:
        - name: keras
          version: "2"
          autoSelect: true
        - name: onnx
          version: "1"
          autoSelect: true
        - name: pytorch
          version: "1"
          autoSelect: true
        - name: tensorflow
          version: "1"
          autoSelect: true
        - name: tensorflow
          version: "2"
          autoSelect: true
        - name: tensorrt
          version: "7"
          autoSelect: true

      protocolVersions:
        - grpc-v2
      multiModel: true

      grpcEndpoint: "port:8085"
      grpcDataEndpoint: "port:8001"

      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 2Gi
      containers:
        - name: triton
          image: nvcr.io/nvidia/tritonserver:24.05-py3
          command:
            - /bin/sh
            - -c
            - |
              #!/bin/sh
              mkdir -p /models/_triton_models
              chmod 775 /models/_triton_models

              exec tritonserver \
                --model-repository=/models/_triton_models \
                --model-control-mode=explicit \
                --strict-model-config=false \
                --strict-readiness=false \
                --allow-http=true \
                --allow-sagemaker=false

          volumeMounts:
            - name: shm
              mountPath: /dev/shm
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: "5"
              memory: 2Gi
          livenessProbe:
            # the server is listening only on 127.0.0.1, so an httpGet probe sent
            # from the kublet running on the node cannot connect to the server
            # (not even with the Host header or host field)
            # exec a curl call to have the request originate from localhost in the
            # container
            exec:
              command:
                - /bin/sh
                - -c
                - |
                  curl \
                    --fail \
                    --silent \
                    --show-error \
                    --max-time "8" \
                    http://localhost:8000/v2/health/live
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 10
      builtInAdapter:
        serverType: triton
        runtimeManagementPort: 8001
        memBufferBytes: 134217728
        modelLoadingTimeoutMillis: 90000
